
---
title: "Habits"
author: Gilles Dauby
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: md_document
---


This document document R scripts and functions used for the analysis presented in the manuscript entitled __A third of the tropical African flora is potentially threatened with extinction__ currently under peer-review.



# Load packages (to install if needed)

```{r, include=TRUE, echo=TRUE, message=F, warning=F}
library(tidyverse)
library(sf)
library(raster)
library(ConR)
library(broom)
library(rredlist)
library(rgbif)
library(doParallel)
library(maps)
```

# Load functions specific to these analyses


# load dataset

```{r, include=TRUE, echo=TRUE, message=F, warning=F}
dataset <- read_csv("data/dataset_rainbio_used.csv")
```


```{r, include=TRUE, echo=TRUE, message=F, warning=F}
dataset
```


# extracting data from gbif for all species 
```{r, include=TRUE, echo=TRUE, message=F, warning=F, eval=F}


species_list <-
  dataset %>%
  distinct(tax_sp_level)

list_data <- as.list(pull(species_list))


#### Searching for gbif occurrences for all species, takes around 5 hours in parallel on 4 cores.
doParallel::registerDoParallel(4) ## define here the number of core on which you want to run the search

system.time(results <-
  plyr::llply(list_data, .fun=function(x) {
    # source("functions_criteria_A.R")
    source("gbif_search_filtered_function.R")
  
    gbif_search_filtered(taxa = x)
  }
    , .progress = "text", .parallel=T, .paropts=list(.packages=c('rgbif','raster','tidyverse', 'ConR', 'sf'))))


doParallel::stopImplicitCluster()

gbif_data <- bind_rows(results)

# write_csv(gbif_data, "gbif.searched.all.taxa.csv")

```

```{r, include=F, echo=F, message=F, warning=F}
gbif_data <- read_csv("data/gbif.searched.all.taxa.csv")
```


This table provide for each species the total number of occurence in gbif, the total number of occupied cells in 10 km cell size grid and the list of Continent where the species is recorded on gbif.

```{r, include=T, echo=F, message=F, warning=F}
gbif_data
```

# load SIG dataset
```{r, include=T, echo=F, message=F, warning=F}
## protected areas network
protected_areas_network <- sf::read_sf("data/NationalParks_filtered_corrected.shp")
## mineral deposits plus a buffer of 10 km around each point
mineral_deposits <- sf::read_sf("data/poly_mineral_deposit.shp")
raster_mayaux <- raster("data/sum.rasters.human.impacted.tif")

### getting a nice and updated world map
world <- maps::map("world", fill=TRUE, plot=FALSE)
IDs <- sapply(strsplit(world$names, ":"), function(x) x[1])
world <- maptools::map2SpatialPolygons(world, IDs=IDs, proj4string=sp::CRS("+proj=longlat +datum=WGS84"))
suppressWarnings(world_map <- broom::tidy(world))
world_id <- tibble(id_nbe=1:nrow(distinct(world_map, id)), id=pull(distinct(world_map, id)))

world_map <- as(world, "sf")
world_map <- world_map %>% 
  add_column(id_nbe=world_id$id_nbe, id=world_id$id)
```



## Preliminary Automated Conservation Assessment following Criterion B

```{r CriterionB-Analysis, eval=F, echo=T, warning=F, message=F}

protected_areas_network_sp <- 
  as(protected_areas_network, "Spatial")

criterion_b <-
  IUCN.eval(dplyr::select(dataset, ddlat, ddlon, tax_sp_level)[1:1000,], 
            protec.areas = protected_areas_network_sp, DrawMap = F, SubPop = F, file_name = "dataset_criterion_B", write_file_option = "csv", parallel = T, NbeCores = 4)


```

```{r, include=F, echo=F, message=F, warning=F}
criterion_b <- read_csv("data/dataset_Criterion_B.csv")
```


```{r, include=T, echo=F, message=F, warning=F}
criterion_b
```

